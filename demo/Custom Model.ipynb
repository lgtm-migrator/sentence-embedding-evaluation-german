{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4743dc34-766e-4e29-9e6f-6bf894f73c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d97215c2-0daa-4d18-9b95-013791d9c3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentence_embedding_evaluation_german as seeg\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch_emb2vec import ConvToVec\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbef9d6-654d-4188-a052-9143061d8cd5",
   "metadata": {},
   "source": [
    "## (1) Instantiate your Embedding model\n",
    "First, you should load your pretrained embedding.\n",
    "\n",
    "Here we will generate a random embedding for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d3cfc31-31ad-441e-869d-9970f7071a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a random embedding\n",
    "emb_dim = 256\n",
    "vocab_sz = 128\n",
    "emb = torch.randn((vocab_sz, emb_dim), requires_grad=False)\n",
    "emb = torch.nn.Embedding.from_pretrained(emb)\n",
    "# assert emb.weight.requires_grad == False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d877f18-a1e3-40ce-bd86-110169b6c8de",
   "metadata": {},
   "source": [
    "## (2a) Specify the preprocessing\n",
    "The `preprocessor` function converts a sentences as string into embedding vectors of numbers.\n",
    "\n",
    "Here we will convert the input strings with a nonsensical approach into IDs for the Embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94c4dc56-c9d8-4e06-801e-3ae869852d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQLEN = 128\n",
    "padid = 128\n",
    "\n",
    "def preprocesser(batch: List[str], params: dict=None) -> List[List[List[float]]]:\n",
    "    \"\"\" Specify your embedding or pretrained encoder here\n",
    "    Paramters:\n",
    "    ----------\n",
    "    params : dict\n",
    "        The params dictionary\n",
    "    batch : List[str]\n",
    "        A list of sentence as string\n",
    "    Returns:\n",
    "    --------\n",
    "    List[List[List[float]]]\n",
    "        A list of embedded sequences\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    for sent in batch:\n",
    "        # encode\n",
    "        try:\n",
    "            ids = torch.tensor([ord(c) % 128 for c in sent])\n",
    "        except:\n",
    "            print(sent)\n",
    "        enc = emb(ids)\n",
    "        # truncate & pad\n",
    "        h = torch.ones(size=(SEQLEN, enc.shape[1]), dtype=torch.int64) * padid\n",
    "        end = min(enc.shape[0], SEQLEN)\n",
    "        try:\n",
    "            h[:end, :] = enc[:end, :].detach()\n",
    "        except Exception as e:\n",
    "            raise Exception(e)\n",
    "        features.append(h)\n",
    "    features = torch.stack(features, dim=0)\n",
    "    features = features.type(torch.float32)\n",
    "    return features.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbf9b68-d721-4221-a9c4-0cdc92ed953e",
   "metadata": {},
   "source": [
    "## (2b) Specify a Customer Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62c2d10d-58be-4428-bdd1-e5abe4dc1b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomClassiferModel(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 embdim: int,\n",
    "                 output_size: int,  # nclasses\n",
    "                 *args, **kwargs):\n",
    "        super(CustomClassiferModel, self).__init__(*args, **kwargs)\n",
    "        # Self-MHA layer (=> this will overfit bcoz tiny datasets!)\n",
    "        #self.mha_pre = torch.nn.LayerNorm(embdim, elementwise_affine=True)\n",
    "        #self.mha_net = torch.nn.MultiheadAttention(\n",
    "        #    embed_dim=embdim, num_heads=8, batch_first=True, bias=False)\n",
    "        # 2D to 1D flattening\n",
    "        self.to1d_pre = torch.nn.LayerNorm(embdim, elementwise_affine=True)\n",
    "        # compute kernel size and output channel dim\n",
    "        self.to1d_net = ConvToVec(\n",
    "            seq_len=SEQLEN, emb_dim=embdim, num_output=256, trainable=False)\n",
    "        self.to1d_act = torch.nn.GELU()\n",
    "        # Final layer\n",
    "        self.final_pre = torch.nn.LayerNorm(256, elementwise_affine=True)\n",
    "        self.final_net = torch.nn.Linear(256, output_size, bias=False)\n",
    "        self.final_act = torch.nn.Softmax(dim=1)\n",
    "        # init params\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self) -> None:\n",
    "        torch.manual_seed(42)\n",
    "        # Self-MHA projections\n",
    "        #for param in self.mha_net.parameters():\n",
    "        #    torch.nn.init.xavier_normal_(param, gain=1.0)\n",
    "        # Final layer\n",
    "        torch.nn.init.xavier_normal_(self.final_net.weight, gain=1.0)\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        # Self-MHA layer\n",
    "        #print(\"Inputs\", inputs.shape)\n",
    "        h = inputs\n",
    "        #h = self.mha_pre(h)\n",
    "        #h, _ = self.mha_net(query=h, value=h, key=h)\n",
    "        #h = h + inputs  # skip-conn\n",
    "        # print(\"MHA\", h.shape)\n",
    "        # 2D to 1D flattening\n",
    "        h = self.to1d_pre(h)\n",
    "        h = self.to1d_net(h)\n",
    "        h = self.to1d_act(h)\n",
    "        #print(\"Flatten\", h.shape)\n",
    "        # Final layer\n",
    "        h = self.final_pre(h)\n",
    "        h = self.final_net(h)\n",
    "        #print(\"Final\", h.shape)\n",
    "        return self.final_act(h)\n",
    "\n",
    "\n",
    "def mymodel(**kwargs):\n",
    "    return CustomClassiferModel(\n",
    "        embdim=kwargs['n_features'],\n",
    "        output_size=kwargs['n_classes'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c4e13e2-a25f-45f7-9b02-f98f0f3adc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num params: 68352\n"
     ]
    }
   ],
   "source": [
    "model = mymodel(n_features=emb_dim, n_classes=3)\n",
    "n = sum([m.numel() for m in model.parameters()])\n",
    "print(f\"Num params: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd310db5-7909-406a-b125-c37325b87105",
   "metadata": {},
   "source": [
    "## (3) Training settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02deec53-b58b-4d5f-ae37-77d69f2b4071",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'datafolder': '../datasets',\n",
    "    'bias': True,\n",
    "    'balanced': True,\n",
    "    'batch_size': 128, \n",
    "    'num_epochs': 5,  # Default: 500\n",
    "    # 'early_stopping': True,\n",
    "    # 'split_ratio': 0.2,  # if early_stopping=True\n",
    "    # 'patience': 5,  # if early_stopping=True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5ed1f1-1bb2-4c61-b1ed-d57569436e48",
   "metadata": {},
   "source": [
    "## (4) Specify downstream tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80e0b877-752c-4bde-b4ac-c51b16496ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All\n",
    "# downstream_tasks = [\n",
    "#     'TOXIC', 'ENGAGE', 'FCLAIM', 'VMWE',\n",
    "#     'OL19-A', 'OL19-B', 'OL19-C',\n",
    "#     'OL18-A', 'OL18-B', \n",
    "#     'ABSD-1', 'ABSD-2', 'ABSD-3',\n",
    "#     'MIO-S', 'MIO-O', 'MIO-I', 'MIO-D', 'MIO-F', 'MIO-P', 'MIO-A',\n",
    "#     'SBCH-L', 'SBCH-S', 'ARCHI', 'LSDC'\n",
    "# ]\n",
    "\n",
    "# Group tasks\n",
    "# downstream_tasks = [\n",
    "#     'ABSD-2', 'MIO-S', 'SBCH-S',  # Sentiment analysis\n",
    "#     'ENGAGE', 'MIO-P',  # engaging/personal\n",
    "#     'FCLAIM', 'MIO-A',  # fact-claim (potential fake news), argumentative, reasoning\n",
    "#     'TOXIC', 'OL19-A', 'OL19-B', 'OL19-C', 'MIO-O', 'MIO-I',  # toxic\n",
    "# ]\n",
    "\n",
    "# Current favorites\n",
    "downstream_tasks = ['FCLAIM', 'VMWE', 'OL19-C', 'ABSD-2', 'MIO-P', 'ARCHI', 'LSDC']\n",
    "downstream_tasks = ['FCLAIM', 'VMWE', 'OL19-C', 'ABSD-2', 'MIO-P', 'ARCHI']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59f4e17-b4a9-4c99-b41d-17a7364d1ead",
   "metadata": {},
   "source": [
    "## (5) Run experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbc1d65f-034e-45eb-9d9c-f8db94deb3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: FCLAIM\n",
      "epoch 1 | loss: 0.714004798577382\n",
      "epoch 2 | loss: 0.6992716720471015\n",
      "epoch 3 | loss: 0.6866543568097628\n",
      "epoch 4 | loss: 0.6757772335639367\n",
      "epoch 5 | loss: 0.6673626349522517\n",
      "Dataset loaded: VMWE\n",
      "epoch 1 | loss: 0.7426156963293369\n",
      "epoch 2 | loss: 0.7321090480455985\n",
      "epoch 3 | loss: 0.7239403472496913\n",
      "epoch 4 | loss: 0.717761639219064\n",
      "epoch 5 | loss: 0.7133696629450872\n",
      "Dataset loaded: OL19-C\n",
      "epoch 1 | loss: 0.7187334559857845\n",
      "epoch 2 | loss: 0.7131192833185196\n",
      "epoch 3 | loss: 0.7092077378183603\n",
      "epoch 4 | loss: 0.7055755704641342\n",
      "epoch 5 | loss: 0.7020894940942526\n",
      "Dataset loaded: ABSD-2\n",
      "epoch 1 | loss: 1.13924005078642\n",
      "epoch 2 | loss: 1.1295296008649625\n",
      "epoch 3 | loss: 1.1202633294620012\n",
      "epoch 4 | loss: 1.111301327222272\n",
      "epoch 5 | loss: 1.1033252940366143\n",
      "Dataset loaded: MIO-P\n",
      "epoch 1 | loss: 0.7111242767926809\n",
      "epoch 2 | loss: 0.6952965533411181\n",
      "epoch 3 | loss: 0.6815511358750833\n",
      "epoch 4 | loss: 0.6689673919935484\n",
      "epoch 5 | loss: 0.6584070647085035\n",
      "Dataset loaded: ARCHI\n",
      "epoch 1 | loss: 1.4123175233399787\n",
      "epoch 2 | loss: 1.4035374899299777\n",
      "epoch 3 | loss: 1.398762891892673\n",
      "epoch 4 | loss: 1.3943602738737249\n",
      "epoch 5 | loss: 1.3904624455640104\n",
      "CPU times: user 13min 36s, sys: 8.21 s, total: 13min 44s\n",
      "Wall time: 7min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = seeg.evaluate(\n",
    "    downstream_tasks=downstream_tasks, \n",
    "    preprocesser=preprocesser, \n",
    "    modelbuilder=mymodel,\n",
    "    verbose=1,\n",
    "    **params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8d70632-f1fa-4b9f-8f55-fa228c36668b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "dat = json.dumps(results, indent=2)\n",
    "# print(dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7e3482-6c13-43aa-8071-d7037e5b4e91",
   "metadata": {},
   "source": [
    "## (6) Display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac84999e-8dae-44c7-8442-371d49a0c60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task | Epochs | N train | N test\n",
      " FCLAIM:     5   3244    944\n",
      "   VMWE:     5   6652   1447\n",
      " OL19-C:     5   1921    930\n",
      " ABSD-2:     5  19432   2555\n",
      "  MIO-P:     5   4668   4668\n",
      "  ARCHI:     5  18809   4743\n"
     ]
    }
   ],
   "source": [
    "print(\"Task | Epochs | N train | N test\")\n",
    "for res in results:\n",
    "    print(f\"{res['task']:>7s}: {res['epochs']:5d} {res['train']['num']:6d} {res['test']['num']:6d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17a9e87f-9401-441f-9b77-959be2860a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Task | train | test\n",
      " FCLAIM:  0.639  0.606\n",
      "   VMWE:  0.675  0.679\n",
      " OL19-C:  0.536  0.528\n",
      " ABSD-2:  0.382  0.359\n",
      "  MIO-P:  0.597  0.581\n",
      "  ARCHI:  0.282  0.253\n"
     ]
    }
   ],
   "source": [
    "metric = 'acc'  # 'f1', 'f1-balanced', 'acc', 'acc-balanced'\n",
    "print('  Task | train | test')\n",
    "for res in results:\n",
    "    print(f\"{res['task']:>7s}: {res['train'][metric]:6.3f} {res['test'][metric]:6.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9fbc769-02c7-4a86-be83-313d7f04a96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Task | train | test\n",
      " FCLAIM:  0.606  0.576\n",
      "   VMWE:  0.505  0.502\n",
      " OL19-C:  0.445  0.445\n",
      " ABSD-2:  0.317  0.290\n",
      "  MIO-P:  0.534  0.512\n",
      "  ARCHI:  0.275  0.249\n"
     ]
    }
   ],
   "source": [
    "metric = 'f1-balanced'  # 'f1', 'f1-balanced', 'acc', 'acc-balanced'\n",
    "print('  Task | train | test')\n",
    "for res in results:\n",
    "    print(f\"{res['task']:>7s}: {res['train'][metric]:6.3f} {res['test'][metric]:6.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55202679-8bee-48b9-8d4a-6728d8c51706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FCLAIM 944\n",
      "{'0': '630', '1': '314'}\n",
      "{'1': '378', '0': '566'}\n",
      "VMWE 1447\n",
      "{'1': '256', '0': '1191'}\n",
      "{'0': '1119', '1': '328'}\n",
      "OL19-C 930\n",
      "{'0': '796', '1': '134'}\n",
      "{'0': '493', '1': '437'}\n",
      "ABSD-2 2555\n",
      "{'1': '1670', '0': '780', '2': '105'}\n",
      "{'1': '972', '2': '841', '0': '742'}\n",
      "MIO-P 4668\n",
      "{'1': '812', '0': '3856'}\n",
      "{'0': '2561', '1': '2107'}\n",
      "ARCHI 4743\n",
      "{'2': '1177', '3': '1176', '1': '1199', '0': '1191'}\n",
      "{'1': '1711', '2': '1034', '3': '975', '0': '1023'}\n"
     ]
    }
   ],
   "source": [
    "# class label distributions (inference)\n",
    "for res in results:\n",
    "    print(res['task'], res['test']['num'])\n",
    "    print(res['test']['distr-test'])\n",
    "    print(res['test']['distr-pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e7f8b38-81c0-477c-a63c-eb46b2a08fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FCLAIM 3244\n",
      "{'0': '2141', '1': '1103'}\n",
      "{'1': '1207', '0': '2037'}\n",
      "VMWE 6652\n",
      "{'1': '1145', '0': '5507'}\n",
      "{'1': '1607', '0': '5045'}\n",
      "OL19-C 1921\n",
      "{'0': '1664', '1': '257'}\n",
      "{'0': '1032', '1': '889'}\n",
      "ABSD-2 19432\n",
      "{'2': '1179', '1': '13208', '0': '5045'}\n",
      "{'2': '6412', '0': '5424', '1': '7596'}\n",
      "MIO-P 4668\n",
      "{'0': '3855', '1': '813'}\n",
      "{'0': '2535', '1': '2133'}\n",
      "ARCHI 18809\n",
      "{'1': '4797', '3': '4407', '2': '4802', '0': '4803'}\n",
      "{'3': '3578', '2': '3626', '1': '7865', '0': '3740'}\n"
     ]
    }
   ],
   "source": [
    "# class label distribution (training)\n",
    "for res in results:\n",
    "    print(res['task'], res['train']['num'])\n",
    "    print(res['train']['distr-train'])\n",
    "    print(res['train']['distr-pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2a7b0b-f295-4b80-a4b5-f2f5c06ef518",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
