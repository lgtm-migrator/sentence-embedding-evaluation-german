{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4743dc34-766e-4e29-9e6f-6bf894f73c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d97215c2-0daa-4d18-9b95-013791d9c3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/uh/projects/paper-293/sentence-embedding-evaluation-german/.venv/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sentence_embedding_evaluation_german as seeg\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch_emb2vec import ConvToVec\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbef9d6-654d-4188-a052-9143061d8cd5",
   "metadata": {},
   "source": [
    "## (1) Instantiate your Embedding model\n",
    "First, you should load your pretrained embedding.\n",
    "\n",
    "Here we will generate a random embedding for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d3cfc31-31ad-441e-869d-9970f7071a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a random embedding\n",
    "emb_dim = 256\n",
    "vocab_sz = 128\n",
    "emb = torch.randn((vocab_sz, emb_dim), requires_grad=False)\n",
    "emb = torch.nn.Embedding.from_pretrained(emb)\n",
    "# assert emb.weight.requires_grad == False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d877f18-a1e3-40ce-bd86-110169b6c8de",
   "metadata": {},
   "source": [
    "## (2a) Specify the preprocessing\n",
    "The `preprocessor` function converts a sentences as string into embedding vectors of numbers.\n",
    "\n",
    "Here we will convert the input strings with a nonsensical approach into IDs for the Embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94c4dc56-c9d8-4e06-801e-3ae869852d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQLEN = 192\n",
    "padid = 128\n",
    "\n",
    "def preprocesser(batch: List[str], params: dict=None) -> List[List[List[float]]]:\n",
    "    \"\"\" Specify your embedding or pretrained encoder here\n",
    "    Paramters:\n",
    "    ----------\n",
    "    params : dict\n",
    "        The params dictionary\n",
    "    batch : List[str]\n",
    "        A list of sentence as string\n",
    "    Returns:\n",
    "    --------\n",
    "    List[List[List[float]]]\n",
    "        A list of embedded sequences\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    for sent in batch:\n",
    "        # encode\n",
    "        try:\n",
    "            ids = torch.tensor([ord(c) % 128 for c in sent])\n",
    "        except:\n",
    "            print(sent)\n",
    "        enc = emb(ids)\n",
    "        # truncate & pad\n",
    "        h = torch.ones(size=(SEQLEN, enc.shape[1]), dtype=torch.int64) * padid\n",
    "        end = min(enc.shape[0], SEQLEN)\n",
    "        try:\n",
    "            h[:end, :] = torch.tensor(enc[:end, :])\n",
    "        except Exception as e:\n",
    "            raise Exception(e)\n",
    "        features.append(h)\n",
    "    features = torch.stack(features, dim=0)\n",
    "    features = features.type(torch.float32)\n",
    "    return features.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbf9b68-d721-4221-a9c4-0cdc92ed953e",
   "metadata": {},
   "source": [
    "## (2b) Specify a Customer Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62c2d10d-58be-4428-bdd1-e5abe4dc1b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomClassiferModel(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 embdim: int,\n",
    "                 output_size: int,  # nclasses\n",
    "                 *args, **kwargs):\n",
    "        super(CustomClassiferModel, self).__init__(*args, **kwargs)\n",
    "        # Self-MHA layer\n",
    "        self.mha_pre = torch.nn.LayerNorm(embdim, elementwise_affine=True)\n",
    "        self.mha_net = torch.nn.MultiheadAttention(\n",
    "            embed_dim=embdim, num_heads=8, batch_first=True, bias=False)\n",
    "        # 2D to 1D flattening\n",
    "        self.to1d_pre = torch.nn.LayerNorm(embdim, elementwise_affine=True)\n",
    "        # compute kernel size and output channel dim\n",
    "        self.to1d_net = ConvToVec(\n",
    "            seq_len=SEQLEN, emb_dim=embdim, num_output=384, trainable=True)\n",
    "        self.to1d_act = torch.nn.GELU()\n",
    "        # Final layer\n",
    "        self.final_pre = torch.nn.LayerNorm(384, elementwise_affine=True)\n",
    "        self.final_net = torch.nn.Linear(384, output_size, bias=False)\n",
    "        self.final_act = torch.nn.Softmax(dim=1)\n",
    "        # init params\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self) -> None:\n",
    "        torch.manual_seed(42)\n",
    "        # Self-MHA projections\n",
    "        for param in self.mha_net.parameters():\n",
    "            torch.nn.init.xavier_normal_(param, gain=1.0)\n",
    "        # Final layer\n",
    "        torch.nn.init.xavier_normal_(self.final_net.weight, gain=1.0)\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        # Self-MHA layer\n",
    "        # print(\"Inputs\", inputs.shape)\n",
    "        h = self.mha_pre(inputs)\n",
    "        h, _ = self.mha_net(query=h, value=h, key=h)\n",
    "        h = h + inputs  # skip-conn\n",
    "        # print(\"MHA\", h.shape)\n",
    "        # 2D to 1D flattening\n",
    "        h = self.to1d_pre(h)\n",
    "        h = self.to1d_net(h)\n",
    "        h = self.to1d_act(h)\n",
    "        # print(\"Flatten\", h.shape)\n",
    "        # Final layer\n",
    "        h = self.final_pre(h)\n",
    "        h = self.final_net(h)\n",
    "        # print(\"Final\", h.shape)\n",
    "        return self.final_act(h)\n",
    "\n",
    "\n",
    "def mymodel(**kwargs):\n",
    "    return CustomClassiferModel(\n",
    "        embdim=kwargs['n_features'],\n",
    "        output_size=kwargs['n_classes'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c4e13e2-a25f-45f7-9b02-f98f0f3adc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num params: 364416\n"
     ]
    }
   ],
   "source": [
    "model = mymodel(n_features=emb_dim, n_classes=3)\n",
    "n = sum([m.numel() for m in model.parameters()])\n",
    "print(f\"Num params: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd310db5-7909-406a-b125-c37325b87105",
   "metadata": {},
   "source": [
    "## (3) Training settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02deec53-b58b-4d5f-ae37-77d69f2b4071",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'datafolder': '../datasets',\n",
    "    'bias': True,\n",
    "    'balanced': True,\n",
    "    'batch_size': 128, \n",
    "    'num_epochs': 3,  # Default: 500\n",
    "    # 'early_stopping': True,\n",
    "    # 'split_ratio': 0.2,  # if early_stopping=True\n",
    "    # 'patience': 5,  # if early_stopping=True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5ed1f1-1bb2-4c61-b1ed-d57569436e48",
   "metadata": {},
   "source": [
    "## (4) Specify downstream tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80e0b877-752c-4bde-b4ac-c51b16496ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All\n",
    "# downstream_tasks = [\n",
    "#     'TOXIC', 'ENGAGE', 'FCLAIM', 'VMWE',\n",
    "#     'OL19-A', 'OL19-B', 'OL19-C',\n",
    "#     'OL18-A', 'OL18-B', \n",
    "#     'ABSD-1', 'ABSD-2', 'ABSD-3',\n",
    "#     'MIO-S', 'MIO-O', 'MIO-I', 'MIO-D', 'MIO-F', 'MIO-P', 'MIO-A',\n",
    "#     'SBCH-L', 'SBCH-S', 'ARCHI', 'LSDC'\n",
    "# ]\n",
    "\n",
    "# Group tasks\n",
    "# downstream_tasks = [\n",
    "#     'ABSD-2', 'MIO-S', 'SBCH-S',  # Sentiment analysis\n",
    "#     'ENGAGE', 'MIO-P',  # engaging/personal\n",
    "#     'FCLAIM', 'MIO-A',  # fact-claim (potential fake news), argumentative, reasoning\n",
    "#     'TOXIC', 'OL19-A', 'OL19-B', 'OL19-C', 'MIO-O', 'MIO-I',  # toxic\n",
    "# ]\n",
    "\n",
    "# Current favorites\n",
    "downstream_tasks = ['FCLAIM', 'VMWE', 'OL19-C', 'ABSD-2', 'MIO-P', 'ARCHI', 'LSDC']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59f4e17-b4a9-4c99-b41d-17a7364d1ead",
   "metadata": {},
   "source": [
    "## (5) Run experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc1d65f-034e-45eb-9d9c-f8db94deb3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/uh/projects/paper-293/sentence-embedding-evaluation-german/.venv/lib/python3.7/site-packages/ipykernel_launcher.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: FCLAIM\n",
      "epoch 1 | loss: 0.6685386781509106\n",
      "epoch 2 | loss: 0.5976097973493429\n",
      "epoch 3 | loss: 0.5702292093863854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/uh/projects/paper-293/sentence-embedding-evaluation-german/.venv/lib/python3.7/site-packages/ipykernel_launcher.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: VMWE\n",
      "epoch 1 | loss: 0.7062330807630832\n",
      "epoch 2 | loss: 0.7167477389940848\n",
      "epoch 3 | loss: 0.6321420148015022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/uh/projects/paper-293/sentence-embedding-evaluation-german/.venv/lib/python3.7/site-packages/ipykernel_launcher.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: OL19-C\n",
      "epoch 1 | loss: 0.7175990007817745\n",
      "epoch 2 | loss: 0.6447488404810429\n",
      "epoch 3 | loss: 0.5736871752887964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/uh/projects/paper-293/sentence-embedding-evaluation-german/.venv/lib/python3.7/site-packages/ipykernel_launcher.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: ABSD-2\n",
      "epoch 1 | loss: 1.0872514538074796\n",
      "epoch 2 | loss: 1.0219420496570437\n",
      "epoch 3 | loss: 0.9905103343097788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/uh/projects/paper-293/sentence-embedding-evaluation-german/.venv/lib/python3.7/site-packages/ipykernel_launcher.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/Users/uh/projects/paper-293/sentence-embedding-evaluation-german/.venv/lib/python3.7/site-packages/ipykernel_launcher.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: MIO-P\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = seeg.evaluate(\n",
    "    downstream_tasks=downstream_tasks, \n",
    "    preprocesser=preprocesser, \n",
    "    modelbuilder=mymodel,\n",
    "    verbose=1,\n",
    "    **params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d70632-f1fa-4b9f-8f55-fa228c36668b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "dat = json.dumps(results, indent=2)\n",
    "# print(dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7e3482-6c13-43aa-8071-d7037e5b4e91",
   "metadata": {},
   "source": [
    "## (6) Display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac84999e-8dae-44c7-8442-371d49a0c60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Task | Epochs | N train | N test\")\n",
    "for res in results:\n",
    "    print(f\"{res['task']:>7s}: {res['epochs']:5d} {res['train']['num']:6d} {res['test']['num']:6d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a9e87f-9401-441f-9b77-959be2860a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'acc'  # 'f1', 'f1-balanced', 'acc', 'acc-balanced'\n",
    "print('  Task | train | test')\n",
    "for res in results:\n",
    "    print(f\"{res['task']:>7s}: {res['train'][metric]:6.3f} {res['test'][metric]:6.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fbc769-02c7-4a86-be83-313d7f04a96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'f1-balanced'  # 'f1', 'f1-balanced', 'acc', 'acc-balanced'\n",
    "print('  Task | train | test')\n",
    "for res in results:\n",
    "    print(f\"{res['task']:>7s}: {res['train'][metric]:6.3f} {res['test'][metric]:6.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55202679-8bee-48b9-8d4a-6728d8c51706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class label distributions (inference)\n",
    "for res in results:\n",
    "    print(res['task'], res['test']['num'])\n",
    "    print(res['test']['distr-test'])\n",
    "    print(res['test']['distr-pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7f8b38-81c0-477c-a63c-eb46b2a08fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class label distribution (training)\n",
    "for res in results:\n",
    "    print(res['task'], res['train']['num'])\n",
    "    print(res['train']['distr-train'])\n",
    "    print(res['train']['distr-pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2a7b0b-f295-4b80-a4b5-f2f5c06ef518",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
